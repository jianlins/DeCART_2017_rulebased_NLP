{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis (1)\n",
    "\n",
    "From the previous notebook, you have seen that our current NLP solution does not get 100% correct. Although it is unrealistic to reach that goal, we definitely can make it closer. \n",
    "\n",
    "This notebook will show you how to analysis errors related to name entity recognition, and guide you through step by step to improve the recall. We will talk about how to improve precision tomorrow.\n",
    "\n",
    "## 1. Locate the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first import libraries\n",
    "import urllib.request\n",
    "import os\n",
    "import codecs\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the classes and functions that we have created in previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Annotation(object):\n",
    "    def __init__(self):\n",
    "        self.start_index = -1\n",
    "        self.end_index = -1\n",
    "        self.type = ''\n",
    "        self.spanned_text = ''\n",
    "        \n",
    "    # adding this so that pyConText's HTML markup can work seamlessly\n",
    "    def getSpan(self):\n",
    "        return (self.start_index, self.end_index)\n",
    "    \n",
    "    def getCategory(self):\n",
    "        # pyConText graph objects actually expect a list here\n",
    "        return [self.type]\n",
    "\n",
    "class AnnotatedDocument(object):\n",
    "    def __init__(self):\n",
    "        self.text = ''\n",
    "        self.annotations = []\n",
    "        self.positive_label = -1\n",
    "        \n",
    "def read_brat_annotations(lines):\n",
    "    annotations = []\n",
    "    # BRAT FORMAT is:\n",
    "    # NUMBER[TAB]TYPE[SPACE]START_INDEX[SPACE]END_INDEX[SPACE]SPANNED_TEXT\n",
    "    for line in lines:\n",
    "        line = str(line)\n",
    "        tab_tokens = line.split('\\t')\n",
    "        space_tokens = tab_tokens[1].split()\n",
    "        anno = Annotation()\n",
    "        anno.spanned_text = tab_tokens[-1]\n",
    "        anno.type = space_tokens[0]\n",
    "        anno.start_index = int(space_tokens[1])\n",
    "        anno.end_index = int(space_tokens[2])\n",
    "        annotations.append(anno)\n",
    "    return annotations       \n",
    "\n",
    "def mark_text(txt,nodes,colors = {\"name\":\"red\",\"pet\":\"blue\"},default_color=\"black\"):\n",
    "    from pyConTextNLP.display.html import __insert_color\n",
    "    # this function had to be copied and modified from pyConTextNLP.display.html.mark_text \n",
    "    # so that the default_color could be passed through\n",
    "    if not nodes:\n",
    "        return txt\n",
    "    else:\n",
    "        n = nodes.pop(-1)\n",
    "        return mark_text(__insert_color(txt,\n",
    "                                        n.getSpan(),\n",
    "                                        colors.get(n.getCategory()[0],default_color)),\n",
    "                         nodes,\n",
    "                         colors=colors,\n",
    "                         # this was not being passed through \n",
    "                        default_color = default_color)\n",
    "    \n",
    "def pneumonia_html_markup(anno_doc):\n",
    "    from pyConTextNLP.display.html import __sort_by_span\n",
    "    # this bit mimics 'mark_document_with_html' from pyConTextNLP.display.html\n",
    "    colors = {}\n",
    "    colors['DOCUMENT_PNEUMONIA_YES'] = 'red'\n",
    "    colors['DOCUMENT_PNEUMONIA_NO'] = 'green'\n",
    "    colors['SPAN_POSITIVE_PNEUMONIA_EVIDENCE'] = 'orange'\n",
    "    default_color = 'red'\n",
    "    html = \"\"\"<p> {0} </p>\"\"\".format(\" \".join([mark_text(anno_doc.text,\n",
    "                                                 __sort_by_span(anno_doc.annotations),\n",
    "                                                 colors=colors,\n",
    "                                                 default_color=default_color)]))\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the errors, first we need to tweek a little bit on the function below: instead of returning a list of values, we need to return the \"*annotated_doc_map*\" where the keys are the document names, and the values are \"*AnnotatedDocument*\"s. Because you may want to see in which documents our prediction got errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_annotations(archive_file, force_redownload = False):\n",
    "    print('Reading annotations from file : ' + archive_file)\n",
    "    filename = archive_file.split('/')[-1]\n",
    "    \n",
    "    if force_redownload or not os.path.isfile(filename):\n",
    "        print('Downloading remote file : '+ archive_file)\n",
    "        urllib.request.urlretrieve(archive_file, filename)\n",
    "    \n",
    "    annotated_doc_map = {}\n",
    "    \n",
    "    print('Opening local file : ' + filename)\n",
    "    z = zipfile.ZipFile(filename, \"r\")\n",
    "    zinfo = z.namelist()\n",
    "    for name in zinfo:\n",
    "        if name.endswith('.txt') or name.endswith('.ann'):\n",
    "            basename = name.split('.')[0]\n",
    "            if basename not in annotated_doc_map:\n",
    "                annotated_doc_map[basename] = AnnotatedDocument()\n",
    "            anno_doc = annotated_doc_map[basename]\n",
    "            # handle text and BRAT annotation files (.ann) differently\n",
    "            if name.endswith('.txt'):\n",
    "                with z.open(name) as f1:\n",
    "                    anno_doc.text = f1.read().decode('utf8')\n",
    "            else:\n",
    "                with z.open(name) as f1:\n",
    "                    # handle this as utf8 or we get back byte arrays\n",
    "                    anno_doc.annotations = read_brat_annotations(codecs.iterdecode(f1, 'utf8'))\n",
    "                    \n",
    "    # now let's finally assign a 0 or 1 to each document based on whether we see our expected type for the pneumonia label\n",
    "    for key, anno_doc in annotated_doc_map.items():\n",
    "        annos = anno_doc.annotations\n",
    "        anno_doc.positive_label = 0\n",
    "        for anno in annos:\n",
    "            if anno.type == 'DOCUMENT_PNEUMONIA_YES':\n",
    "                anno_doc.positive_label = 1\n",
    "#Note: here is the tweak we need to make                    \n",
    "    return annotated_doc_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tweak the function **calculate_prediction_metrics** to list the difference--errors, instead of calculate the measurements:\n",
    "\n",
    "Question before we move on:\n",
    "\n",
    "Why we only care *false negatives* for now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_false_negatives(gold_docs, prediction_function):\n",
    "    fn_docs={}\n",
    "    for doc_name, gold_doc in gold_docs.items():\n",
    "        gold_label=gold_doc.positive_label;\n",
    "        pred_label = prediction_function(gold_doc.text)\n",
    "        if gold_label==1 and pred_label==0:\n",
    "            fn_docs[doc_name]=gold_doc            \n",
    "    return fn_docs     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Display errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put everything together to display errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "annotated_docs = read_annotations('https://github.com/burgersmoke/DeCART_2017_rulebased_NLP/raw/master/data/BRAT/BratTestArchive.zip')\n",
    "print('Total Annotated Documents : {0}'.format(len(annotated_docs)))\n",
    "\n",
    "fn=list_false_negatives(annotated_docs, keyword_classifier.predict)\n",
    "\n",
    "docs=list(fn.keys());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show one document a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if i<len(docs):\n",
    "    print (docs[i])\n",
    "\n",
    "    anno_doc=fn[docs[i]]\n",
    "\n",
    "    display(HTML(pneumonia_html_markup(anno_doc).replace('\\n', '<br>')))\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More efficient review:\n",
    "Not convenient to read? Let's try snippet view instead. Now we need to make another function to replace \"*pneumonia_html_markup*\". \n",
    "\n",
    "Although we measuring the document level annotation, we will focus on mention level (\"**SPAN_POSITIVE_PNEUMONIA_EVIDENCE**\") error analyses. Because the later is where the errors originate from.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def snippets_markup(annotated_doc_map):\n",
    "    html = [\"<html>\",\"<table width=100% >\",\n",
    "            \"<col style=\\\"width:25%\\\"><col style=\\\"width:75%\\\">\"\n",
    "            \"<tr><th style=\\\"text-align:center\\\">document name</th><th style=\\\"text-align:center\\\">Snippets</th>\"]\n",
    "    for doc_name, anno_doc in annotated_doc_map.items():\n",
    "        html.extend(snippet_markup(doc_name,anno_doc))\n",
    "    html.append(\"</table>\")\n",
    "    html.append(\"</html>\")\n",
    "    return ''.join(html) \n",
    "\n",
    "\n",
    "def snippet_markup(doc_name,anno_doc):\n",
    "    from pyConTextNLP.display.html import __sort_by_span\n",
    "    from pyConTextNLP.display.html import __insert_color\n",
    "    html=[]\n",
    "    color= 'orange'    \n",
    "    window_size=50    \n",
    "    html.append(\"<tr>\")\n",
    "    html.append(\"<td style=\\\"text-align:left\\\">{0}</td>\".format(doc_name))\n",
    "    html.append(\"<td></td>\")\n",
    "    html.append(\"</tr>\")\n",
    "    for anno in anno_doc.annotations:\n",
    "        if anno.type == 'SPAN_POSITIVE_PNEUMONIA_EVIDENCE':\n",
    "#           make sure the our snippet will be cut inside the text boundary\n",
    "            begin=anno.start_index-window_size\n",
    "            end=anno.end_index+window_size\n",
    "            begin=begin if begin>0 else 0\n",
    "            end=end if end<len(anno_doc.text) else len(anno_doc.text)    \n",
    "#           render a highlighted snippet\n",
    "            cell=__insert_color(anno_doc.text[begin:end],[anno.start_index-begin,anno.end_index-end],color)\n",
    "#           add the snippet into table\n",
    "            html.append(\"<tr>\")\n",
    "            html.append(\"<td></td>\")\n",
    "            html.append(\"<td style=\\\"text-align:left\\\">{0}</td>\".format(cell))\n",
    "            html.append(\"</tr>\") \n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(snippets_markup(fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Now what?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
