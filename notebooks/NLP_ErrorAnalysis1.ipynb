{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis (1)\n",
    "\n",
    "From the previous notebook, you have seen that our current NLP solution does not get 100% correct. Although it is unrealistic to reach that goal, we definitely can make it closer. \n",
    "\n",
    "This notebook will show you how to analysis errors related to name entity recognition, and guide you through step by step to improve the recall. We will talk about how to improve precision tomorrow.\n",
    "\n",
    "## 1. Locate the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first import libraries\n",
    "import urllib.request\n",
    "import os\n",
    "import codecs\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML,IFrame\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the classes and functions that we have created in previous notebook.\n",
    "Note: we are going to use *read_doc_annotations* (return a map with document name as the key, and annotations as the value) instead of *read_annotations* (return a list of documents' annotations), so that we list the errors with the corresponding document name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nlp_pneumonia_utils import Annotation\n",
    "from nlp_pneumonia_utils import AnnotatedDocument\n",
    "from nlp_pneumonia_utils import read_brat_annotations\n",
    "from nlp_pneumonia_utils import read_doc_annotations\n",
    "from nlp_pneumonia_utils import calculate_prediction_metrics\n",
    "from nlp_pneumonia_utils import mark_text\n",
    "from nlp_pneumonia_utils import pneumonia_html_markup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tweak the function **calculate_prediction_metrics** to list the difference--errors, instead of calculate the measurements:\n",
    "\n",
    "Question before we move on:\n",
    "\n",
    "Why we only care *false negatives* for now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_false_negatives(gold_docs, prediction_function):\n",
    "    fn_docs={}\n",
    "    for doc_name, gold_doc in gold_docs.items():\n",
    "        gold_label=gold_doc.positive_label;\n",
    "        pred_label = prediction_function(gold_doc.text)\n",
    "        if gold_label==1 and pred_label==0:\n",
    "            fn_docs[doc_name]=gold_doc            \n",
    "    return fn_docs     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Display errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put everything together to display errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KeywordClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.keywords = set()\n",
    "    def predict(self, text):\n",
    "        prediction = 0\n",
    "        for keyword in self.keywords:\n",
    "            if keyword in text:\n",
    "                prediction = 1\n",
    "        return prediction\n",
    "    \n",
    "keyword_classifier = KeywordClassifier()\n",
    "# let's load in some manual keywords...\n",
    "keyword_classifier.keywords.add('pneumonia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "annotated_docs = read_doc_annotations('pneumonia_brat_full_set1.zip')\n",
    "print('Total Annotated Documents : {0}'.format(len(annotated_docs)))\n",
    "\n",
    "fn=list_false_negatives(annotated_docs, keyword_classifier.predict)\n",
    "\n",
    "docs=list(fn.keys());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show one document a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if i<len(docs):\n",
    "    print (docs[i])\n",
    "\n",
    "    anno_doc=fn[docs[i]]\n",
    "\n",
    "    display(HTML(pneumonia_html_markup(anno_doc).replace('\\n', '<br>')))\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More efficient review:\n",
    "Not convenient to read? Let's try snippet view instead. Now we need to make another function to replace \"*pneumonia_html_markup*\". \n",
    "\n",
    "Although we measuring the document level annotation, we will focus on mention level (\"**SPAN_POSITIVE_PNEUMONIA_EVIDENCE**\") error analyses. Because the later is where the errors originate from.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def snippets_markup(annotated_doc_map):\n",
    "    html = [\"<html>\",\"<table width=100% >\",\n",
    "            \"<col style=\\\"width:25%\\\"><col style=\\\"width:75%\\\">\"\n",
    "            \"<tr><th style=\\\"text-align:center\\\">document name</th><th style=\\\"text-align:center\\\">Snippets</th>\"]\n",
    "    for doc_name, anno_doc in annotated_doc_map.items():\n",
    "        html.extend(snippet_markup(doc_name,anno_doc))\n",
    "    html.append(\"</table>\")\n",
    "    html.append(\"</html>\")\n",
    "    return ''.join(html) \n",
    "\n",
    "\n",
    "def snippet_markup(doc_name,anno_doc):\n",
    "    from pyConTextNLP.display.html import __sort_by_span\n",
    "    from pyConTextNLP.display.html import __insert_color\n",
    "    html=[]\n",
    "    color= 'blue'    \n",
    "    window_size=50    \n",
    "    html.append(\"<tr>\")\n",
    "    html.append(\"<td style=\\\"text-align:left\\\">{0}</td>\".format(doc_name))\n",
    "    html.append(\"<td></td>\")\n",
    "    html.append(\"</tr>\")\n",
    "    for anno in anno_doc.annotations:\n",
    "        if anno.type == 'SPAN_POSITIVE_PNEUMONIA_EVIDENCE':\n",
    "#           make sure the our snippet will be cut inside the text boundary\n",
    "            begin=anno.start_index-window_size\n",
    "            end=anno.end_index+window_size\n",
    "            begin=begin if begin>0 else 0\n",
    "            end=end if end<len(anno_doc.text) else len(anno_doc.text)    \n",
    "#           render a highlighted snippet\n",
    "            cell=__insert_color(anno_doc.text[begin:end],[anno.start_index-begin,anno.end_index-end],color)\n",
    "#           add the snippet into table\n",
    "            html.append(\"<tr>\")\n",
    "            html.append(\"<td></td>\")\n",
    "            html.append(\"<td style=\\\"text-align:left\\\">{0}</td>\".format(cell))\n",
    "            html.append(\"</tr>\") \n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(HTML(snippets_markup(fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Now what?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
