{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import codecs\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading annotations from file : https://github.com/burgersmoke/DeCART_2017_rulebased_NLP/raw/master/data/BRAT/BratTestArchive.zip\n",
      "Opening local file : BratTestArchive.zip\n",
      "Total Annotated Documents : 100\n",
      "Total Positive Pneumonia Documents : 3\n"
     ]
    }
   ],
   "source": [
    "class Annotation(object):\n",
    "    def __init__(self):\n",
    "        self.start_index = -1\n",
    "        self.end_index = -1\n",
    "        self.type = ''\n",
    "        self.spanned_text = ''\n",
    "\n",
    "class AnnotatedDocument(object):\n",
    "    def __init__(self):\n",
    "        self.text = ''\n",
    "        self.annotations = []\n",
    "        self.positive_label = -1\n",
    "        \n",
    "def read_brat_annotations(lines):\n",
    "    annotations = []\n",
    "    # BRAT FORMAT is:\n",
    "    # NUMBER[TAB]TYPE[SPACE]START_INDEX[SPACE]END_INDEX[SPACE]SPANNED_TEXT\n",
    "    for line in lines:\n",
    "        line = str(line)\n",
    "        tab_tokens = line.split('\\t')\n",
    "        space_tokens = tab_tokens[1].split()\n",
    "        anno = Annotation()\n",
    "        anno.spanned_text = tab_tokens[-1]\n",
    "        anno.type = space_tokens[0]\n",
    "        anno.start_index = int(space_tokens[1])\n",
    "        anno.end_index = int(space_tokens[2])\n",
    "        annotations.append(anno)\n",
    "    return annotations\n",
    "        \n",
    "def read_annotations(archive_file, force_redownload = False):\n",
    "    print('Reading annotations from file : ' + archive_file)\n",
    "    filename = archive_file.split('/')[-1]\n",
    "    \n",
    "    if force_redownload or not os.path.isfile(filename):\n",
    "        print('Downloading remote file : '+ archive_file)\n",
    "        urllib.request.urlretrieve(archive_file, filename)\n",
    "    \n",
    "    annotated_doc_map = {}\n",
    "    \n",
    "    print('Opening local file : ' + filename)\n",
    "    z = zipfile.ZipFile(filename, \"r\")\n",
    "    zinfo = z.namelist()\n",
    "    for name in zinfo:\n",
    "        if name.endswith('.txt') or name.endswith('.ann'):\n",
    "            basename = name.split('.')[0]\n",
    "            if basename not in annotated_doc_map:\n",
    "                annotated_doc_map[basename] = AnnotatedDocument()\n",
    "            anno_doc = annotated_doc_map[basename]\n",
    "            # handle text and BRAT annotation files (.ann) differently\n",
    "            if name.endswith('.txt'):\n",
    "                with z.open(name) as f1:\n",
    "                    anno_doc.text = f1.read().decode('utf8')\n",
    "            else:\n",
    "                with z.open(name) as f1:\n",
    "                    # handle this as utf8 or we get back byte arrays\n",
    "                    anno_doc.annotations = read_brat_annotations(codecs.iterdecode(f1, 'utf8'))\n",
    "                    \n",
    "    # now let's finally assign a 0 or 1 to each document based on whether we see our expected type for the pneumonia label\n",
    "    for key, anno_doc in annotated_doc_map.items():\n",
    "        annos = anno_doc.annotations\n",
    "        anno_doc.positive_label = 0\n",
    "        for anno in annos:\n",
    "            if anno.type == 'DOCUMENT_PNEUMONIA_YES':\n",
    "                anno_doc.positive_label = 1\n",
    "                    \n",
    "    return annotated_doc_map.values()\n",
    "\n",
    "def calculate_prediction_metrics(gold_docs, prediction_function):\n",
    "    gold_labels = [x.positive_label for x in gold_docs]\n",
    "    pred_labels = []\n",
    "    for gold_doc in gold_docs:\n",
    "        pred_label = prediction_function(gold_doc.text)\n",
    "        pred_labels.append(pred_label)\n",
    "        \n",
    "    # now let's use scikit-learn to compute some metrics\n",
    "    precision = sklearn.metrics.precision_score(gold_labels, pred_labels)\n",
    "    recall = sklearn.metrics.recall_score(gold_labels, pred_labels)\n",
    "    f1 = sklearn.metrics.f1_score(gold_labels, pred_labels)\n",
    "    # let's use Pandas to make a confusion matrix for us\n",
    "    confusion_matrix_df = pd.crosstab(pd.Series(gold_labels, name = 'Actual'), \n",
    "                                      pd.Series(pred_labels, name = 'Predicted'))\n",
    "    \n",
    "    print('Precision : {0}'.format(precision))\n",
    "    print('Recall : {0}'.format(recall))\n",
    "    print('F1: {0}'.format(f1))\n",
    "    \n",
    "    print('Confusion Matrix : ')\n",
    "    print(confusion_matrix_df)\n",
    "    \n",
    "annotated_docs = read_annotations('https://github.com/burgersmoke/DeCART_2017_rulebased_NLP/raw/master/data/BRAT/BratTestArchive.zip')\n",
    "print('Total Annotated Documents : {0}'.format(len(annotated_docs)))\n",
    "\n",
    "total_positives = 0\n",
    "for anno_doc in annotated_docs:\n",
    "    if anno_doc.positive_label:\n",
    "        total_positives += 1\n",
    "    \n",
    "print('Total Positive Pneumonia Documents : {0}'.format(total_positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting and validating the naive baseline of always predicting NO\n",
      "Precision : 0.0\n",
      "Recall : 0.0\n",
      "F1: 0.0\n",
      "Confusion Matrix : \n",
      "Predicted   0\n",
      "Actual       \n",
      "0          97\n",
      "1           3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# let's first illustrate a naive baseline by always prediction NO pneumonia (i.e. 0)\n",
    "def naive_negative_pneumonia_prediction(text):\n",
    "    return 0\n",
    "    \n",
    "print('Predicting and validating the naive baseline of always predicting NO')\n",
    "calculate_prediction_metrics(annotated_docs, naive_negative_pneumonia_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting and validating the naive baseline of always predicting YES\n",
      "Precision : 0.03\n",
      "Recall : 1.0\n",
      "F1: 0.058252427184466014\n",
      "Confusion Matrix : \n",
      "Predicted   1\n",
      "Actual       \n",
      "0          97\n",
      "1           3\n"
     ]
    }
   ],
   "source": [
    "# let's first illustrate a naive baseline by always prediction NO pneumonia (i.e. 0)\n",
    "def naive_positive_pneumonia_prediction(text):\n",
    "    return 1\n",
    "    \n",
    "print('Predicting and validating the naive baseline of always predicting YES')\n",
    "calculate_prediction_metrics(annotated_docs, naive_positive_pneumonia_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting and validating the naive PNEUMONIA keyword baseline\n",
      "Precision : 0.046511627906976744\n",
      "Recall : 0.6666666666666666\n",
      "F1: 0.08695652173913045\n",
      "Confusion Matrix : \n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          56  41\n",
      "1           1   2\n"
     ]
    }
   ],
   "source": [
    "# now let's try a very naive simulated baseline to assign positive Pneumonia anytime the work \"pneumonia\" appears in a document\n",
    "def naive_pneumonia_keyword_prediction(text):\n",
    "    if 'pneumonia' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "print('Predicting and validating the naive PNEUMONIA keyword baseline')\n",
    "calculate_prediction_metrics(annotated_docs, naive_pneumonia_keyword_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting and validating a classifier which uses some keywords above\n",
      "Precision : 0.0\n",
      "Recall : 0.0\n",
      "F1: 0.0\n",
      "Confusion Matrix : \n",
      "Predicted   0  1\n",
      "Actual          \n",
      "0          89  8\n",
      "1           3  0\n"
     ]
    }
   ],
   "source": [
    "# let's try another one where we use a class to store some \"keywords\"\n",
    "class KeywordClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.keywords = set()\n",
    "    def predict(self, text):\n",
    "        prediction = 0\n",
    "        for keyword in self.keywords:\n",
    "            if keyword in text:\n",
    "                prediction = 1\n",
    "        return prediction\n",
    "    \n",
    "keyword_classifier = KeywordClassifier()\n",
    "# let's load in some manual keywords...\n",
    "keyword_classifier.keywords.add('positive')\n",
    "keyword_classifier.keywords.add('confirmed')\n",
    "keyword_classifier.keywords.add('probable')\n",
    "\n",
    "print('Predicting and validating a classifier which uses some keywords above')\n",
    "calculate_prediction_metrics(annotated_docs, keyword_classifier.predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
