{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Mention-Level Annotations to Document Classification\n",
    "\n",
    "## 1. Why do we need document classification?\n",
    "\n",
    "Think about a case with multiple mentions in one document. How do we decide the document level conclusion when these mentions have \"conflicted\" information? For example, \n",
    "\n",
    ">Small **left pleural effusion**. **Right pleural effusion can be excluded**.\n",
    "\n",
    "In this example, should we conclude that the report indicates pneumonia or does not indicate pneumonia?\n",
    "\n",
    "There are many other situations that we need to draw a document level conclusion based on multiple mention level annotations. Certainly, we can train a machine learning classifier to accomplish this task, which you will learn in another class. But here we are going to learn how to do it in rule-based way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Restore from where we are using pyConText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import everything that we will need\n",
    "import pyConTextNLP\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "from pyConTextNLP.itemData import itemData\n",
    "from pyConTextNLP.display.html import mark_document_with_html\n",
    "import os\n",
    "import os.path\n",
    "import radnlp\n",
    "import radnlp.rules as rules\n",
    "import radnlp.utils as utils\n",
    "import radnlp.split as split\n",
    "import radnlp.view as rview\n",
    "import radnlp.schema as schema\n",
    "import radnlp.classifier as classifier\n",
    "from radnlp.data import classrslts\n",
    "from nlp_pneumonia_utils import Annotation\n",
    "from nlp_pneumonia_utils import AnnotatedDocument\n",
    "from nlp_pneumonia_utils import read_brat_annotations\n",
    "from nlp_pneumonia_utils import read_annotations\n",
    "from nlp_pneumonia_utils import calculate_prediction_metrics\n",
    "from nlp_pneumonia_utils import mark_text\n",
    "from nlp_pneumonia_utils import pneumonia_html_markup\n",
    "from IPython.display import display, HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's just consider the example at the beginning as a document,\n",
    "# and run pyConText to get markups\n",
    "\n",
    "report = \"Right pleural effusion can be excluded. Likely small left pleural effusion. \"\n",
    "\n",
    "targets = itemData([\"effusion\", \"SPAN_POSITIVE_PNEUMONIA_EVIDENCE\", r\"effusion[s]?\", \"\"])\n",
    "\n",
    "modifiers = pyConTextNLP.itemData.instantiateFromCSVtoitemData(os.path.join(os.getcwd(),'KB/lexical_kb_05042016.tsv'))\n",
    "\n",
    "markup = utils.mark_report(split.get_sentences(report),\n",
    "                         modifiers,\n",
    "                         targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To confirm what we get from pyConText\n",
    "print(markup.getDocumentGraph())\n",
    "        \n",
    "\n",
    "context_html = pyConTextNLP.display.html.\\\n",
    "    mark_document_with_html(markup, colors = {\"span_positive_pneumonia_evidence\": \"blue\"})\n",
    "display(HTML(context_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use RadNLP to define the rules for document classification\n",
    "\n",
    "RadNLP is a simple rule based document classification package. Let's consider the document classification problem as a **Q**(question)&**A**(answer) task. For example, to answer the question \"does the report indicates pneumonia?\", we can do it in two steps:\n",
    "\n",
    "1. For each mention-level annotation, what answer we can get . (Let's name it as classification rules)\n",
    "2. Among all the answers we get from step 1, which one should we prioritize as the document level conclusion. (Let's name it as schema rules)\n",
    "\n",
    "Now let's start from something really simple:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Define classification rules\n",
    "\n",
    "In the example above, we found both a positive indication of pneumonia and a negated one. Let represent the question with a name 'DISEASE_STATE'. If it is a positive mention, we conclude 'DISEASE_STATE=1', otherwise, 'DISEASE_STATE=0'. \n",
    "\n",
    "Intuitively, let's define: whenever we find a positive indication, conclude the report as pneumonia positive, no matter whether a negated indication exists or not.\n",
    "\n",
    "```Python\n",
    "@CLASSIFICATION_RULE,DISEASE_STATE,RULE,0,DEFINITE_NEGATED_EXISTENCE,,,,,,,,,\n",
    "```\n",
    "Let's break down this rule to interpret it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Element | Meaning |\n",
    "|:--- |:--- |\n",
    "| @CLASSIFICATION_RULE | Indicate this is a classification rule (another two types of rules are not covered here)|\n",
    "| DISEASE_STATE | This rule is to sign 'DISEASE_STATE' a value|\n",
    "| 0 | 'DISEASE_STATE' will be signed value '0' when the rule matched|\n",
    "| DEFINITE_NEGATED_EXISTENCE | If find a 'DEFINITE_NEGATED_EXISTENCE' modifier, consider a match|\n",
    "| ,,,,, | You can add more match conditions separted by comma |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need one more rule to give a default value, when no rules are matched. Because the pyConText is designed to identify context clues that not positive, so by default (no clue is found) the value should be positive.\n",
    "```Python\n",
    "@CLASSIFICATION_RULE,DISEASE_STATE,DEFAULT,1,,,,,,,,,,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Define schema rules\n",
    "\n",
    "Since we have two mention-level annotations in the example, we will get two answers for question 'DISEASE_STATE': 0 and 1. Let's define rules to priorities '1' over '0'.\n",
    "```Python\n",
    "1,Negative,DISEASE_STATE == 0\n",
    "2,Positive,DISEASE_STATE == 1\n",
    "```\n",
    "Again, let's break down one of the rules to interpret it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Element | Meaning |\n",
    "|:--- |:--- |\n",
    "| 1 | priority score is '1'|\n",
    "| Negative | Let's name this answer as 'Negative'. It won't affect the output, but help us to read the answer |\n",
    "| DISEASE_STATE == 0 | If 'DISEASE_STATE == 0' |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two rules give each answer of 'DISEASE_STATE' a score. If 'DISEASE_STATE == 0', score=1. If 'DISEASE_STATE == 1', score=2. The answer with a higher score will be prioritized.\n",
    "\n",
    "\n",
    "#### Question: Why don't we just use  the  1 and 0 as the score?\n",
    "\n",
    "### (3) Let's put rules into code\n",
    "\n",
    "These rules have already been saved in two files for testing. Classification rules are saved in [classificationRules.csv](KB/classificationRules.csv). Schema rules are saved in [schema.csv](KB/schema.csv). We will try to edit them later. For now, let's just read them as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let add these two rules into code\n",
    "cls_rules = rules.read_rules(os.path.join(os.getcwd(),'KB/classificationRules.csv'))\n",
    "print('Classification Rules:\\n\\n'+str(cls_rules[0]))\n",
    "\n",
    "\n",
    "schema_rules = schema.read_schema(os.path.join(os.getcwd(),'KB/schema.csv'))\n",
    "print(\"\\nSchema:\\n\\n\"+str(_schema))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function illustrates how the classification answers will be drawn for each mention-level annotation,\n",
    "# and how document schema answer will be updated from each mention-level answer\n",
    "def classify_document_targets(doc,\n",
    "                              classification_rules,\n",
    "                              category_rules,\n",
    "                              severity_rules,schema,\n",
    "                              neg_filters=[\"definite_negated_existence\",\n",
    "                                           \"probable_negated_existence\"],\n",
    "                              exclusion_categories=[\"QUALITY_FEATURE\",\n",
    "                                                    \"ARTIFACT\"]):\n",
    "    \"\"\"\n",
    "    Look at the targets and their modifiers to get an overall\n",
    "    classification for the document_markup\n",
    "    \"\"\"\n",
    "    rslts = {}\n",
    "\n",
    "    qualityInducedUncertainty = False\n",
    "    try:\n",
    "        g = doc.getDocumentGraph()\n",
    "    except:\n",
    "        g = doc\n",
    "    targets = [n[0] for n in g.nodes(data=True)\n",
    "               if n[1].get(\"category\", \"\") == 'target']\n",
    "    \n",
    "\n",
    "    if targets:        \n",
    "        for t in targets:   \n",
    "#           print a target annotation\n",
    "            print(t)\n",
    "            severity_values = []\n",
    "            current_rslts = {}\n",
    "            current_category = t.getCategory()\n",
    "            \n",
    "\n",
    "#           for rk in classification_rules:\n",
    "#           iterate across all the classification_rules\n",
    "            \n",
    "            current_rslts = \\\n",
    "                {rk:utils.generic_classifier(g,\n",
    "                                             t,\n",
    "                                             classification_rules[rk])\n",
    "                 for rk in classification_rules}  \n",
    "#           print current results\n",
    "            print('The classification answer for this mention: \\n\\t'+str(current_rslts))\n",
    "\n",
    "            current_category = t.categoryString()\n",
    "            # now need to compare current_rslts to rslts\n",
    "            # to select most Positive\n",
    "            docr = classifier.classify_result(current_rslts, _schema)\n",
    "            print('The schema answer for this mention = '+str(docr))\n",
    "            trslts = rslts.get(current_category, (-1, '', []))\n",
    "            if trslts[0] < docr:\n",
    "                trslts = (docr, t.getXML(), severity_values)\n",
    "            rslts[current_category] = trslts\n",
    "            print ('Current document level schema answer ='+str(trslts[0])+'\\n')\n",
    "            \n",
    "        else:\n",
    "            if t.isA('QUALITY_FEATURE'):\n",
    "                qualityInducedUncertainty = True\n",
    "            else:\n",
    "                # if non-negated artifacts call uncertain\n",
    "                if not utils.modifies(g, t, neg_filters):\n",
    "                    qualityInducedUncertainty = True\n",
    "\n",
    "    return rslts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's see how the answer is gotten step by step\n",
    "\n",
    "clssfy =   classify_document_targets(markup, cls_rules[0],cls_rules[1],cls_rules[2],schema_rules)\n",
    "\n",
    "print('Finally we got answer: '+str(clssfy['span_positive_pneumonia_evidence'][0]))                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# without step by step print, you can use RadNLP's classifier function directly:\n",
    "\n",
    "clssfy =   classifier.classify_document_targets(markup, cls_rules[0],cls_rules[1],cls_rules[2],schema_rules)\n",
    "print('Finally we got answer: '+str(clssfy['span_positive_pneumonia_evidence'][0]))                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (4) Let's try to switch the sentences in the example\n",
    "See what happens. Does the order of mention-level annotation affects final conclusion?\n",
    "\n",
    "### (5) Let's try to add one more question \n",
    "\n",
    "Is the document's conclusion certain or uncertain?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
